{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3564f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c99c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_parquet(path, train, drop_rows=False, columns=None):\n",
    "    \"\"\"\n",
    "    split_paraquet takes a path to a parquet file and splits it into multiple numpy array files (.npy)\n",
    "    It splits it by sequence_id\n",
    "\n",
    "    Parameters:\n",
    "        path (string)    : Path to parquet file \n",
    "        train (bool)     : Indicates whether the parquet files are for training (True) or are supplemental (False)\n",
    "        drop_rows (bool) : Whether to drop NA rows or not. Default: False\n",
    "        columns (list)   : List of columns t\n",
    "\n",
    "    Returns:\n",
    "        Max frame count for this parquet file\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    df = pd.read_parquet(path,columns=columns)\n",
    "    # Get rid of face and pose data\n",
    "    #df.drop(df.columns[1:469], axis=1, inplace=True)\n",
    "    # df.drop(df.columns[22:55], axis=1, inplace=True)\n",
    "    # df.drop(df.columns[43:511], axis=1, inplace=True)\n",
    "    # df.drop(df.columns[64:97], axis=1, inplace=True)\n",
    "    # df.drop(df.columns[85:553], axis=1, inplace=True)\n",
    "    # df.drop(df.columns[106:139], axis=1, inplace=True)\n",
    "    # df.drop(df.columns[85:127], axis=1, inplace=True)\n",
    "\n",
    "    grouped_df = df.groupby(['sequence_id'], dropna=False)\n",
    "\n",
    "    # try:\n",
    "    #     print(\"split_files does not exists. Creating directory\")\n",
    "    #     os.mkdir(\"split_files\")\n",
    "    # except:\n",
    "    #     print(\"split_files already exists. Skipping directory creation\")\n",
    "    # finally:\n",
    "    #     print(\"Splitting parquet file and saving in split_files\")\n",
    "\n",
    "    if(train):\n",
    "        if (not os.path.isdir(\"train_split_files\")):\n",
    "            print(\"train_split_files does not exists. Creating directory\")\n",
    "            os.mkdir(\"train_split_files\")\n",
    "        else:\n",
    "           print(\"train_split_files already exists. Skipping directory creation\") \n",
    "    else:\n",
    "        if (not os.path.isdir(\"suppl_split_files\")):\n",
    "            print(\"suppl_split_files does not exists. Creating directory\")\n",
    "            os.mkdir(\"suppl_split_files\")\n",
    "        else:\n",
    "           print(\"suppl_split_files already exists. Skipping directory creation\") \n",
    "\n",
    "    for name, subset_df in grouped_df:\n",
    "        subset_df.dropna(axis=1, how='all', inplace=True)\n",
    "        subset_df.interpolate(inplace=True)\n",
    "\n",
    "        if(subset_df.count(axis=0)[0] >= max_length):\n",
    "            max_length = subset_df.count(axis=0).iloc[0]\n",
    "        \n",
    "        if(drop_rows):\n",
    "            subset_df.dropna(axis=0, subset=subset_df.columns[1:], how='all', inplace=True)\n",
    "        \n",
    "        subsdf_np = subset_df.to_numpy()\n",
    "        subsdf_size = np.shape(subsdf_np)\n",
    "        zeros_mat = np.zeros((784-subsdf_size[0], subsdf_size[1]))\n",
    "        subsdf_np = np.vstack(subsdf_np, zeros_mat)\n",
    "\n",
    "        # while(subset_df.count(axis=0).iloc[0] < 784):\n",
    "        #     # zeros_row = pd.Series(np.zeros(subset_df.count(axis=1, numeric_only=True).iloc[0]), name=name)\n",
    "        #     # subset_df = subset_df.append(zeros_row)\n",
    "        #     subset_df.loc[len(subset_df)] = 0\n",
    "        #     last = subset_df.index[-1]\n",
    "        #     subset_df = subset_df.rename(index={last: str(name)})\n",
    "\n",
    "        # print(subset_df.count(axis=1, numeric_only=True).iloc[0])\n",
    "    \n",
    "        # print(subset_df)\n",
    "\n",
    "        if (train):\n",
    "            np.save(f\"train_split_files/{name}\",subsdf_np)\n",
    "        else:\n",
    "            np.save(f\"suppl_split_files/{name}\", subsdf_np)\n",
    "        \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4c60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_dist(a, b):\n",
    "    \"\"\"\n",
    "    lev_dist returns the Levenshtein Distance between two strings\n",
    "\n",
    "    Parameters:\n",
    "        a (string) : First string \n",
    "        b (string) : Second string\n",
    "\n",
    "    Returns:\n",
    "        Levenshtein Distance (int) \n",
    "    \"\"\"\n",
    "\n",
    "    distance_matrix = np.zeros((len(a)+1, len(b)+1), np.int8)\n",
    "    \n",
    "    for i in range(1, len(a)+1):\n",
    "        distance_matrix[i][0] = i\n",
    "    \n",
    "    for i in range(1, len(b)+1):\n",
    "        distance_matrix[0][i] = i\n",
    "\n",
    "    for i in range(1, len(a)+1):\n",
    "        for j in range(1, len(b) + 1):\n",
    "\n",
    "            if(a[i - 1] == b[j-1]):\n",
    "                distance_matrix[i][j] = distance_matrix[i-1][j-1]\n",
    "            else:\n",
    "                distance_matrix[i][j] = min(distance_matrix[i][j - 1], distance_matrix[i - 1][j], distance_matrix[i - 1][j - 1]) + 1\n",
    "\n",
    "\n",
    "    return distance_matrix[-1][-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21135ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEL_FEATURES = ['x_right_hand_0','y_right_hand_0',\n",
    "                'x_right_hand_1','y_right_hand_1',\n",
    "                'x_right_hand_2','y_right_hand_2',\n",
    "                'x_right_hand_3','y_right_hand_3',\n",
    "                'x_right_hand_4','y_right_hand_4',\n",
    "                'x_right_hand_5','y_right_hand_5',\n",
    "                'x_right_hand_6','y_right_hand_6',\n",
    "                'x_right_hand_7','y_right_hand_7',\n",
    "                'x_right_hand_8','y_right_hand_8',\n",
    "                'x_right_hand_9','y_right_hand_9',\n",
    "                'x_right_hand_10','y_right_hand_10',\n",
    "                'x_right_hand_11','y_right_hand_11',\n",
    "                'x_right_hand_12','y_right_hand_12',\n",
    "                'x_right_hand_13','y_right_hand_13',\n",
    "                'x_right_hand_14','y_right_hand_14',\n",
    "                'x_right_hand_15','y_right_hand_15',\n",
    "                'x_right_hand_16','y_right_hand_16',\n",
    "                'x_right_hand_17','y_right_hand_17',\n",
    "                'x_right_hand_18','y_right_hand_18',\n",
    "                'x_right_hand_19','y_right_hand_19',\n",
    "                'x_right_hand_20','y_right_hand_20',\n",
    "                'x_left_hand_0','y_left_hand_0',\n",
    "                'x_left_hand_1','y_left_hand_1',\n",
    "                'x_left_hand_2','y_left_hand_2',\n",
    "                'x_left_hand_3','y_left_hand_3',\n",
    "                'x_left_hand_4','y_left_hand_4',\n",
    "                'x_left_hand_5','y_left_hand_5',\n",
    "                'x_left_hand_6','y_left_hand_6',\n",
    "                'x_left_hand_7','y_left_hand_7',\n",
    "                'x_left_hand_8','y_left_hand_8',\n",
    "                'x_left_hand_9','y_left_hand_9',\n",
    "                'x_left_hand_10','y_left_hand_10',\n",
    "                'x_left_hand_11','y_left_hand_11',\n",
    "                'x_left_hand_12','y_left_hand_12',\n",
    "                'x_left_hand_13','y_left_hand_13',\n",
    "                'x_left_hand_14','y_left_hand_14',\n",
    "                'x_left_hand_15','y_left_hand_15',\n",
    "                'x_left_hand_16','y_left_hand_16',\n",
    "                'x_left_hand_17','y_left_hand_17',\n",
    "                'x_left_hand_18','y_left_hand_18',\n",
    "                'x_left_hand_19','y_left_hand_19',\n",
    "                'x_left_hand_20','y_left_hand_20']\n",
    "\n",
    "\n",
    "files = os.listdir(\"parquet_files/train_landmarks\")\n",
    "max_size_train = 0\n",
    "max_size_suppl = 0\n",
    "\n",
    "for file_name in files:\n",
    "     temp_size = split_parquet(path=f\"parquet_files/train_landmarks/{file_name}\", train = True, columns = SEL_FEATURES)\n",
    "     \n",
    "     if(temp_size >= max_size_train):\n",
    "          max_size_train = temp_size\n",
    "\n",
    "files = os.listdir(\"parquet_files/supplemental_landmarks\")\n",
    "for file_name in files:\n",
    "     temp_size = split_parquet(path=f\"parquet_files/supplemental_landmarks/{file_name}\", train = False, columns = SEL_FEATURES)\n",
    "     \n",
    "     if(temp_size >= max_size_suppl):\n",
    "          max_size_suppl = temp_size\n",
    "\n",
    "print(\"Max length of train files: {}\".format(max_size_train))\n",
    "print(\"Max length of supplemental files: {}\".format(max_size_suppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a087d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of train files: 784\n",
      "Max length of supplemental files: 620\n"
     ]
    }
   ],
   "source": [
    "print(\"Max length of train files: {}\".format(max_size_train))\n",
    "print(\"Max length of supplemental files: {}\".format(max_size_suppl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
